{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "## Capstone Proposal\n",
    "Sergio Noviello\n",
    "September 25th, 2018\n",
    "\n",
    "## Definition\n",
    "\n",
    "Cervical cancer is a cause of death in women worldwide. In females, it is the 14th most common cancer. The mortality rates were drastically reduced with the introduction of the smear test. However, it is still important for doctors to visually review cervical images in order to correctly classify the cervix type. An incorrect classification will result in a treatment that will be ineffective and will also be very expensive. \n",
    "\n",
    "This project is based on a competition that was launched on Kaggle a year ago. \n",
    "\n",
    "\n",
    "## Problem statement\n",
    "\n",
    "The goal of this project is to develop a deep learning model that is capable to classify cervical images and predict the probability of each image to be of type 1, 2 or 3.\n",
    "This model will be very helpful for healthcare providers in identifying patients with a cervix of type 2 or type 3 that will lead to further testing.\n",
    "\n",
    "\n",
    "The steps required are as follows: \n",
    "\n",
    "* Download the train and test set from Kaggle.com\n",
    "* Split the training set into train and validation set\n",
    "* Convert images into multi-dimensional arrays\n",
    "* Augment data by flipping, rotating and changing the offset of the images in the training set\n",
    "* Build a deep learning model using a pre-trained model (ResNet50) \n",
    "* Evaluate the model using the validation set\n",
    "* Classify the images in the test set\n",
    "\n",
    "## Metrics\n",
    "\n",
    "I have used the multi-class logarithmic loss as evaluation metric. For each image, a set of predicted probabilities (one for every class) needs to be calculated. The formula is shown in Figure 1:\n",
    "\n",
    "**Fig.1**\n",
    "![Fig.1](notes/multi_log_loss.png)\n",
    "               \n",
    "N is the number of images in the test set, M is the number of categories, log is the natural logarithm, (y_{ij}) is 1 if observation (i) belongs to class (j) and 0 otherwise, and pij is the predicted probability that observation i belongs to class j.\n",
    " \n",
    "## Exploratory Data Analysis\n",
    "\n",
    "#### number of images in the whole dataset\n",
    "\n",
    "The training set consists of 8218 images, divided in 3 classes as shown below. \n",
    "53% of the images are in type_2, 29.5% from type_2 and 17.5% from type_1 (Figure 2)\n",
    "\n",
    "\n",
    "|Type|Base|Additional|Total|\n",
    "|----|----|----------|-----|\n",
    "|type 1|251|1191|**1442**|\n",
    "|type 2|782|3567|**4349**|\n",
    "|type 3|451|1976|**2472**|\n",
    "\n",
    "\n",
    "**Fig.2**\n",
    "![Fig.2](notes/eda1.png)\n",
    "\n",
    "#### image sizes in the train set\n",
    "There are 8 types of image size, the majority are 4128x3096 (Figure 3)\n",
    "\n",
    "**Fig.3**\n",
    "![Fig.3](notes/eda2.png)\n",
    "\n",
    "#### image sizes in the test set\n",
    "The test dataset has a similar composition in terms of image sizes compared to the training set (Figure 4)\n",
    "\n",
    "**Fig.4**\n",
    "![Fig.4](notes/eda3.png)\n",
    "\n",
    "\n",
    "#### Offsets and angles\n",
    "The relevant part of the image is not always centered and also the angle can vary as shown in Figure 5. \n",
    "\n",
    "In this project I have used data augmentation to make minor alterations to the existing dataset. Minor changes such as flips or translations or rotations.\n",
    "\n",
    "**Fig.5**\n",
    "![Fig.5](notes/eda4.png)\n",
    "\n",
    "\n",
    "## CNN Architecture\n",
    "\n",
    "### Benchmark model\n",
    "\n",
    "A simple model was created as benchmark. I have used one convolutional layer with 3x3 filters followed by a relu activation layer. Finally I have flattened the layer into a monodimensional vector and added a fully connected layer which is responsible to assign probabilities for each class using the softmax algorithm. \n",
    "\n",
    "```python\n",
    "def build_model():    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), input_shape=(IMAGE_SIZE, IMAGE_SIZE, CHANNELS)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.summary()\n",
    "    return model\n",
    "```\n",
    "\n",
    "**Fig.6**\n",
    "![Fig.6](notes/benchmark_model.png)\n",
    "\n",
    "### Evaluate benchmark model\n",
    "\n",
    "After training this simple model I have then evaluated the total loss against the validation set. \n",
    "The loss was calculated using the multi-class logloss function\n",
    "\n",
    "```python\n",
    "def logloss_mc(y_true, y_prob, epsilon=1e-15):\n",
    "    \"\"\" Multiclass logloss\n",
    "    This function is not officially provided by Kaggle, so there is no\n",
    "    guarantee for its correctness.\n",
    "    https://github.com/ottogroup/kaggle/blob/master/benchmark.py\n",
    "    \"\"\"\n",
    "\n",
    "    y_prob = y_prob / y_prob.sum(axis=1).reshape(-1, 1)\n",
    "    y_prob = np.maximum(epsilon, y_prob)\n",
    "    y_prob = np.minimum(1 - epsilon, y_prob)\n",
    "\n",
    "    y = [y_prob[i, j] for (i, j) in enumerate(y_true)]\n",
    "    ll = - np.mean(np.log(y))\n",
    "    return ll```\n",
    "    \n",
    "The total loss was: **11.512925148010254**\n",
    "\n",
    "### Tuning the model \n",
    "\n",
    "I have added enhancements to the benchmark model gradually and I have calculated the total loss after each step to see if they would improve the score. \n",
    "\n",
    "The first improvement was to use a technique called 'transfer learning'. Instead of building the model from scratch I have used the weights of a pre-trained model called ResNet50 that is included in the Keras library. \n",
    "\n",
    "The model is trained on the Imagenet dataset. Imagenet is a database of about 1.5 million images each containing multiple labelled objects. This technique has been proved to be effective because the lower layers of these pre-trained models already contain many generic features such edge recognition and color detectors, features that are common in different datasets. \n",
    "\n",
    "```python\n",
    "def build_model():\n",
    "    base_model = ResNet50(include_top=False, \n",
    "                          weights='imagenet', \n",
    "                          input_tensor=None, \n",
    "                          input_shape=(IMAGE_SIZE, IMAGE_SIZE, CHANNELS))\n",
    "    x = Flatten()(base_model.output)\n",
    "    output = Dense(3, activation='softmax')(x)\n",
    "    model = Model(inputs=base_model.input, outputs=output)\n",
    "    model.summary()\n",
    "    return model\n",
    "    ```\n",
    "\n",
    "The architecture of the model is quite complex and illustrated in Figure 7.\n",
    "\n",
    "**Fig.7**\n",
    "![Fig.7](notes/final_model.png)\n",
    "\n",
    "\n",
    "#### Pre-processing and data augmentation\n",
    "\n",
    "In this step I have split the dataset into training (80%) and validation set (20%). The reason for this, is that it is considered good practice to evaluate a model on data that is different to the data used in the training process. \n",
    "\n",
    "The images were resized to 200x200 using the library **opencv** and converted to numpy arrays. Considering the size of these multi-dimensional arrays is not efficient to store them in memory, therefore I have used python generators. \n",
    "Generators are functions that behave like iterators, object that can be iterated upon. The benefit is the lazy evaluation, iterators donâ€™t compute the value of each item when instantiated. They only compute it when you ask for it.\n",
    "\n",
    "```python\n",
    "image = cv2.imread(file_path)\n",
    "image = cv2.resize(image, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "image = image.astype(np.float32) / 255.0\n",
    "X[cnt, :, :, :] = image\n",
    "class_index = labels.index(image_class)\n",
    "Y[cnt, class_index] = 1\n",
    "cnt += 1\n",
    "if cnt == batch_size:\n",
    "    yield (X, Y)\n",
    "```\n",
    "\n",
    "Another important aspect when training a neural network is to standardize the data to have zero mean and unit variance. \n",
    "During the training of a neural network the initial inputs are multiplied by the weights and added to the biases.\n",
    "Some parameters are shared in a neural network and, if they are not scaled and in the same range, for some part of the image a weight will appear big and to another it will be too small. \n",
    "\n",
    "For this reason I have used the ImageDataGenerator class from Keras, which by default have the parameters featurewise_center and featurewise_std_normalization set to true. \n",
    "\n",
    "The same class also allows you to augment the data by flipping, rotating or shifting horizontally or vertically the offset. \n",
    "\n",
    "The parameters I have used as as follows: \n",
    "\n",
    "|Parameter|Value|\n",
    "|---------|-----|\n",
    "|rotation_range|25|\n",
    "|width_shift_range|0.1|\n",
    "|height_shift_range|0.1|\n",
    "|shear_range|0.2|\n",
    "\n",
    "```python\n",
    "def create_data_generators(validation_set = False): \n",
    "    data_generator = ImageDataGenerator(rescale=1./255., \n",
    "                                        rotation_range=25, \n",
    "                                        width_shift_range=0.1, \n",
    "                                        height_shift_range=0.1, \n",
    "                                        shear_range=0.2,\n",
    "                                        horizontal_flip=True, \n",
    "                                        fill_mode=\"nearest\")\n",
    "\n",
    "    return data_generator.flow_from_directory(TR_DIR, \n",
    "                                              target_size=(IMAGE_SIZE, IMAGE_SIZE), \n",
    "                                              shuffle=True, \n",
    "                                              seed=SEED,\n",
    "                                              class_mode='categorical', \n",
    "                                              batch_size=BATCH_SIZE)\n",
    "   \n",
    "  \n",
    "```\n",
    "\n",
    "### Training the model \n",
    "\n",
    "During the training of this model I have noticed a gap between train and validation loss which is usually an indication of overfitting (Figure 8). \n",
    "\n",
    "**Fig.8**\n",
    "![Fig.8](notes/overfitting.png)\n",
    "\n",
    "#### Reduce overfitting\n",
    "\n",
    "I have decided to add to my model a fully connected layer followed by a dropout layer. Dropout is a regularization technique for reducing overfitting. \n",
    "\n",
    "During training some nodes, chosen with 1-p probability, are left out the network. \n",
    "This helps preventing overfitting because it adds a penalty to the loss function so that it does not learn interdependent set of features weights. \n",
    "\n",
    "```python\n",
    "def build_model():\n",
    "    base_model = ResNet50(include_top=False, \n",
    "                          weights='imagenet', \n",
    "                          input_tensor=None, \n",
    "                          input_shape=(IMAGE_SIZE, IMAGE_SIZE, CHANNELS))\n",
    "    \n",
    "    x = Flatten()(base_model.output)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output = Dense(3, activation='softmax')(x)\n",
    "    model = Model(inputs=base_model.input, outputs=output)\n",
    "    \n",
    "    model.summary()\n",
    "    return model\n",
    "    ```\n",
    "\n",
    "\n",
    "#### Optimizers and learning rate\n",
    "\n",
    "When training a neural network the learning rate can be fixed or it can be reduced as the training progresses. \n",
    "For this I have chosen an adaptive learning rate method called **Adam**. The name is derived from _adaptive moment estimation_.\n",
    "\n",
    "Adam computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradient. \n",
    "\n",
    "Instead of adapting the parameter learning rates based on the average first moment as in RMSProp, Adam  makes use of the average of the second moments of the gradients. \n",
    "\n",
    "For the initial learning rate I have initially selected a lower value of **1.0e-4** but I noticed that the model was converging slowly, so I have decided to increase the learning rate to **1.0e-2**. \n",
    "\n",
    "For the other parameters I have used the default values:\n",
    "\n",
    "|Parameter|Value|Description|\n",
    "|---------|-----|-----------|\n",
    "|beta_1|0.9|the exponential decay rate for the first moment estimates|\n",
    "|beta_2|0.9999|the exponential decay rate for the second-moment estimates|\n",
    "|epsilon|1e-08|a small number to prevent any division by zero|\n",
    "\n",
    "\n",
    "```python\n",
    "def compile_model(model):\n",
    "    opt4 = optimizers.Adam(lr=LEARN_RATE, \n",
    "                           beta_1=0.9, \n",
    "                           beta_2=0.999, \n",
    "                           epsilon=1e-08, \n",
    "                           decay=0.0)\n",
    "    \n",
    "    model.compile(optimizer=opt4, loss='categorical_crossentropy', metrics=['accuracy'])  \n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "#### Batch size\n",
    "\n",
    "Batches are used because, during the training process of a neural network, is not efficient to pass the entire dataset at once.\n",
    "Dividing the number of images in the training set by the batch size will give us the number of interations needed to completed one epoch. One epoch is complete when the entire dataset is passed forward and backward through the network.\n",
    "\n",
    "For my model I have chosen a batch size of 32. I have tried a higher batch size but I noticed that it was slightly overfitting the data. \n",
    "\n",
    "For the number of epochs I started with 30 and I used the class ModelCheckpoint from Keras to check at the end of each epoch if the validation loss was improving. I have noticed that after 5 epochs the validation loss did not improve, therefore I have decided to set the number of epochs to five. \n",
    "\n",
    "I have trained the model on the cloud using an Amazon EC2 instance with GPU for quicker computational power. \n",
    "The training process took nearly 5 hours. \n",
    "\n",
    "Figure 9 shows a visualization of the train and validation loss during the training of the model.\n",
    "\n",
    "**Fig.9**\n",
    "![Fig.9](notes/training.png)\n",
    "\n",
    "### Conclusions\n",
    "\n",
    "The total loss of the final model against the validation set is **1.32852661209**. \n",
    "\n",
    "I have also used the model to make predictions against the test set and submitted the predictions to Kaggle.com\n",
    "Figure 10 shows a score of **1.00197** which would have resulted in position 170 on the private leaderboard.  \n",
    "\n",
    "**Fig.10**\n",
    "![Fig.10](notes/lboard2.png)\n",
    "\n",
    "\n",
    "### Improvements\n",
    "\n",
    "In order to achieve better results the model should be trained for longer than 5 epochs on a more powerful hardware. \n",
    "\n",
    "### References\n",
    "\n",
    "* [kaggle competition](https://www.kaggle.com/c/intel-mobileodt-cervical-cancer-screening)\n",
    "* [cervix types classification](https://kaggle2.blob.core.windows.net/competitions/kaggle/6243/media/Cervix%20types%20clasification.pdf)\n",
    "* [keras documentation](https://keras.io/)\n",
    "* [log loss](https://github.com/ottogroup/kaggle/blob/master/benchmark.py)\n",
    "* [split data](https://github.com/keras-team/keras/issues/5862)\n",
    "* [data augmentation](https://machinelearningmastery.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
